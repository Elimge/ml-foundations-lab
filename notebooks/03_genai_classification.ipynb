{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91207fca",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Notebook: `03_genai_classification.ipynb`\n",
    "\n",
    "## Overview\n",
    "\n",
    "In previous notebooks, we:\n",
    "- Generated a synthetic dataset\n",
    "- Trained and evaluated a classical Logistic Regression model\n",
    "\n",
    "In this notebook, we take a different perspective:\n",
    "- No training phase is required\n",
    "- The model relies entirely on prompt engineering and reasoning\n",
    "- Predictions are generated dynamically via an external LLM\n",
    "\n",
    "We will:\n",
    "1. Define an ambiguous fruit sample\n",
    "2. Query a local LLM (Llama 3 via Ollama)\n",
    "3. Analyze the modelâ€™s reasoning, confidence, and latency\n",
    "\n",
    "This experiment helps illustrate when and why GenAI may (or may not) be appropriate for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045b7c6",
   "metadata": {},
   "source": [
    "## LLM-Based Classification (Zero-Shot)\n",
    "\n",
    "In this section, we classify a single fruit instance using a **Large Language Model**.\n",
    "\n",
    "Key characteristics of this approach:\n",
    "- The LLM has **no access to the dataset**\n",
    "- All domain knowledge is provided explicitly via the prompt\n",
    "- The model performs **zero-shot reasoning**\n",
    "- The output includes both a prediction and an explanation\n",
    "\n",
    "This setup allows us to compare interpretability, flexibility, and latency against the classical ML approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518e19f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing fruit: {'weight_g': 175, 'skin_roughness': 6.0}\n",
      "Querying Llama 3 (this may take a few seconds)...\n",
      "\n",
      "Response time: 38.45 seconds\n",
      "--- AI OPINION ---\n",
      "{\n",
      "  \"fruit\": \"Avocado\",\n",
      "  \"confidence\": \"high\",\n",
      "  \"reason\": \"The weight of 175g is closer to the average avocado weight (200g) than the mango weight (150g), and the skin roughness of 6.0/10 is also more typical of avocados (8/10) than mangoes (3/10).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- 1. LOAD DATA AND CONTEXT ---\n",
    "# Load the processed dataset used by the classical model\n",
    "df = pd.read_csv('../data/processed/fruits_dataset.csv')\n",
    "\n",
    "# Select a challenging or ambiguous fruit\n",
    "# We intentionally choose an \"uncertain\" case:\n",
    "# intermediate weight and medium skin roughness\n",
    "mysterious_fruit = {'weight_g': 175, 'skin_roughness': 6.0}\n",
    "\n",
    "print(f\"Analyzing fruit: {mysterious_fruit}\")\n",
    "\n",
    "# --- APPROACH 1: USING LLAMA 3 AS A CLASSIFIER ---\n",
    "\n",
    "def query_llama(weight, roughness):\n",
    "    # Special Docker URL to access Ollama running on the host machine\n",
    "    url = \"http://host.docker.internal:11434/api/generate\"\n",
    "    \n",
    "    # Prompt Engineering:\n",
    "    # We explicitly describe the synthetic universe and its rules\n",
    "    prompt = (\n",
    "        f\"You are an expert agronomist. There are two possible fruits:\\n\"\n",
    "        f\"1. Avocado: Usually weighs around 200g and has rough skin (8/10 scale).\\n\"\n",
    "        f\"2. Mango: Usually weighs around 150g and has smooth skin (3/10 scale).\\n\\n\"\n",
    "        f\"I have a fruit that weighs {weight}g and has a skin roughness of {roughness}/10.\\n\"\n",
    "        f\"Based on this information, which fruit is it most likely to be?\\n\"\n",
    "        f\"Respond using this exact JSON format: \"\n",
    "        f\"{{'fruit': 'name', 'confidence': 'high/medium/low', 'reason': 'brief explanation'}}.\"\n",
    "    )\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama3\",  # Any model available in Ollama can be used here\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        # Llama 3 supports native JSON mode, which is ideal for programmatic usage\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return json.loads(response.json()['response'])\n",
    "        else:\n",
    "            return f\"Error: {response.text}\"\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            \"Connection error. Is Ollama running on the host machine?\\n\"\n",
    "            f\"Details: {e}\"\n",
    "        )\n",
    "\n",
    "# --- EXECUTION ---\n",
    "print(\"Querying Llama 3 (this may take a few seconds)...\")\n",
    "start_time = time.time()\n",
    "llama_result = query_llama(\n",
    "    mysterious_fruit['weight_g'],\n",
    "    mysterious_fruit['skin_roughness']\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nResponse time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"--- AI OPINION ---\")\n",
    "print(json.dumps(llama_result, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f2260",
   "metadata": {},
   "source": [
    "## Results and Observations\n",
    "\n",
    "The LLM returns:\n",
    "- A predicted fruit type\n",
    "- A qualitative confidence level\n",
    "- A short natural-language explanation\n",
    "\n",
    "In addition, we measure the response time, which is a critical factor when considering GenAI systems in production environments.\n",
    "\n",
    "This experiment demonstrates that:\n",
    "- LLMs can act as reasoning-based classifiers\n",
    "- Predictions are explainable but non-deterministic\n",
    "- Latency and external dependencies must be carefully considered\n",
    "\n",
    "In the next steps, this approach can be compared directly against the classical model in terms of:\n",
    "- Accuracy\n",
    "- Consistency\n",
    "- Performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
